{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        },
        {
          "sourceId": 11371,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 5171
        },
        {
          "sourceId": 11372,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 5388
        }
      ],
      "dockerImageVersionId": 30673,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Prompt Gemma like a Pro",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashispapu/LLMs/blob/main/Prompt_Gemma_like_a_Pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This Notebook is just for me to check a bit Gemma model.\n",
        "Will just load Gemma and demonstrate some very simple use cases.\n",
        "I intend to learn from this experience so that I can then build someting a bit more complex."
      ],
      "metadata": {
        "id": "cwxIgFj2EsDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Keras NLP and Keras"
      ],
      "metadata": {
        "id": "pfoFn-38EsDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U keras>=3"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-03-31T11:07:26.752807Z",
          "iopub.execute_input": "2024-03-31T11:07:26.753623Z",
          "iopub.status.idle": "2024-03-31T11:07:57.339447Z",
          "shell.execute_reply.started": "2024-03-31T11:07:26.753588Z",
          "shell.execute_reply": "2024-03-31T11:07:57.33844Z"
        },
        "trusted": true,
        "id": "LCawA-XqEsDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ],
      "metadata": {
        "id": "XsdWf_B9EsDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras_nlp\n",
        "import os\n",
        "\n",
        "# Select the desired backend for Keras. Options: \"jax\", \"tensorflow\", or \"torch\".\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Adjust as needed.\n",
        "\n",
        "# Specific to the JAX backend, this setting helps avoid memory fragmentation, ensuring more efficient resource use.\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T11:08:35.46814Z",
          "iopub.execute_input": "2024-03-31T11:08:35.469188Z",
          "iopub.status.idle": "2024-03-31T11:08:49.450389Z",
          "shell.execute_reply.started": "2024-03-31T11:08:35.469114Z",
          "shell.execute_reply": "2024-03-31T11:08:49.449481Z"
        },
        "trusted": true,
        "id": "2bJZIjqVEsDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Gemma Causal LLM through Keras NLP"
      ],
      "metadata": {
        "id": "Av5a8ub4EsDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T11:08:49.452187Z",
          "iopub.execute_input": "2024-03-31T11:08:49.453027Z",
          "iopub.status.idle": "2024-03-31T11:09:57.076989Z",
          "shell.execute_reply.started": "2024-03-31T11:08:49.452988Z",
          "shell.execute_reply": "2024-03-31T11:09:57.075983Z"
        },
        "trusted": true,
        "id": "Nto1BQRDEsDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check Gemma parameters"
      ],
      "metadata": {
        "id": "CQNnfJjHEsDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T11:09:57.078835Z",
          "iopub.execute_input": "2024-03-31T11:09:57.079206Z",
          "iopub.status.idle": "2024-03-31T11:09:57.114302Z",
          "shell.execute_reply.started": "2024-03-31T11:09:57.079174Z",
          "shell.execute_reply": "2024-03-31T11:09:57.113462Z"
        },
        "trusted": true,
        "id": "8Os3h2iyEsDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define and run a simple prompt (geography question)"
      ],
      "metadata": {
        "id": "4PEUI0WhEsDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an AI assistant designed to answer simple questions.\n",
        "Please restrict your answer to the exact question asked.\n",
        "Think step by step, use careful reasoning.\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "response = gemma_lm.generate(prompt.format(question=\"What is the surface temperature of the Moon?\"), max_length=256)\n",
        "print(response)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T11:10:10.28719Z",
          "iopub.execute_input": "2024-03-31T11:10:10.287539Z",
          "iopub.status.idle": "2024-03-31T11:10:32.073659Z",
          "shell.execute_reply.started": "2024-03-31T11:10:10.287513Z",
          "shell.execute_reply": "2024-03-31T11:10:32.07269Z"
        },
        "trusted": true,
        "id": "0SpEkTc_EsDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Format the output"
      ],
      "metadata": {
        "id": "AbJJKVWUEsDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def colorize_text(text):\n",
        "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n",
        "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
        "    return text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T11:17:01.454994Z",
          "iopub.execute_input": "2024-03-31T11:17:01.455862Z",
          "iopub.status.idle": "2024-03-31T11:17:01.461396Z",
          "shell.execute_reply.started": "2024-03-31T11:17:01.455828Z",
          "shell.execute_reply": "2024-03-31T11:17:01.460322Z"
        },
        "trusted": true,
        "id": "73buHEiSEsDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = gemma_lm.generate(prompt.format(question=\"What is the surface temperature of the Moon?\"), max_length=256)\n",
        "display(Markdown(colorize_text(response)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T11:14:08.477049Z",
          "iopub.execute_input": "2024-03-31T11:14:08.477894Z",
          "iopub.status.idle": "2024-03-31T11:14:09.304208Z",
          "shell.execute_reply.started": "2024-03-31T11:14:08.477861Z",
          "shell.execute_reply": "2024-03-31T11:14:09.303286Z"
        },
        "trusted": true,
        "id": "3iVcQZjGEsDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try another one (math problem)"
      ],
      "metadata": {
        "id": "E7vnlJg9EsDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an AI assistant designed to write simple Python code.\n",
        "Please answer with the listing of the Python code.\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "response = gemma_lm.generate(prompt.format(question=\"Please write a function in Python to calculate the area of a circle of radius r\"), max_length=256)\n",
        "display(Markdown(colorize_text(response)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T11:14:18.668149Z",
          "iopub.execute_input": "2024-03-31T11:14:18.668651Z",
          "iopub.status.idle": "2024-03-31T11:14:20.898046Z",
          "shell.execute_reply.started": "2024-03-31T11:14:18.66862Z",
          "shell.execute_reply": "2024-03-31T11:14:20.897092Z"
        },
        "trusted": true,
        "id": "j-gAPj9FEsDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple parameters questions\n",
        "\n",
        "I create here a prompt with multiple parameters and I ask a variety of questions."
      ],
      "metadata": {
        "id": "w-H3fddvEsDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an AI assistant designed to answer questions with parameters.\n",
        "Return the answer formated nicely, for example with bullet points.\n",
        "Question: What are the best {number} {items} from {place}?\n",
        "Answer:\n",
        "\"\"\"\n",
        "response = gemma_lm.generate(prompt.format(\n",
        "    number=\"3\",\n",
        "    items=\"food\",\n",
        "    place=\"France\"\n",
        "    ),\n",
        "    max_length=256)\n",
        "display(Markdown(colorize_text(response)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T11:14:33.368247Z",
          "iopub.execute_input": "2024-03-31T11:14:33.368889Z",
          "iopub.status.idle": "2024-03-31T11:14:33.85638Z",
          "shell.execute_reply.started": "2024-03-31T11:14:33.368857Z",
          "shell.execute_reply": "2024-03-31T11:14:33.855373Z"
        },
        "trusted": true,
        "id": "Ha7QVG9qEsDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = gemma_lm.generate(prompt.format(\n",
        "    number=\"five\",\n",
        "    items=\"attractions\",\n",
        "    place=\"Italy\"\n",
        "    ),\n",
        "    max_length=256)\n",
        "display(Markdown(colorize_text(response)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T11:14:40.285347Z",
          "iopub.execute_input": "2024-03-31T11:14:40.285736Z",
          "iopub.status.idle": "2024-03-31T11:14:41.034644Z",
          "shell.execute_reply.started": "2024-03-31T11:14:40.285706Z",
          "shell.execute_reply": "2024-03-31T11:14:41.033619Z"
        },
        "trusted": true,
        "id": "ysxM6z1NEsDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = gemma_lm.generate(prompt.format(\n",
        "    number=\"two\",\n",
        "    items=\"places to retire\",\n",
        "    place=\"Spain\"\n",
        "    ),\n",
        "    max_length=256)\n",
        "display(Markdown(colorize_text(response)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T11:14:46.621137Z",
          "iopub.execute_input": "2024-03-31T11:14:46.621843Z",
          "iopub.status.idle": "2024-03-31T11:14:47.778273Z",
          "shell.execute_reply.started": "2024-03-31T11:14:46.621811Z",
          "shell.execute_reply": "2024-03-31T11:14:47.777326Z"
        },
        "trusted": true,
        "id": "CcIBJjfVEsD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reasoning like Einstein will do"
      ],
      "metadata": {
        "id": "L0vxzbj9EsD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are a math professor, smart but cool.\n",
        "Background: A train traveling from Bucharest to Ploiesti (60 km distance) has the speed of 60 km/h.\n",
        "The train starts in Bucharest and travels until Ploiesti, once, only in this direction.\n",
        "A swallow, flying with 90 km/h, fly from Ploiesti to the moving train.\n",
        "When it reaches the train, the swallow flies back toward Ploiesti,\n",
        "ahead of the train. At Ploiesti turns again back and continues to fly back and forth\n",
        "(between the train approaching Ploiesti and Ploiesti) until the train reaches Ploiesti.\n",
        "The swallow will fly continously all the time the train is traveling from Bucharest to Ploiesti.\n",
        "Reasoning: Think step by step. Explain your reasoning.\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "response = gemma_lm.generate(prompt.format(question=\"How many kilometers will travel totally the swallow?\"),\n",
        "    max_length=512)\n",
        "display(Markdown(colorize_text(response)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T11:16:09.981117Z",
          "iopub.execute_input": "2024-03-31T11:16:09.982032Z",
          "iopub.status.idle": "2024-03-31T11:16:15.725506Z",
          "shell.execute_reply.started": "2024-03-31T11:16:09.981995Z",
          "shell.execute_reply": "2024-03-31T11:16:15.724552Z"
        },
        "trusted": true,
        "id": "mIr2dClIEsD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "\n",
        "## Using gemma_2b_en\n",
        "\n",
        "Initially, I used the model `gemma_2b_en` and I was not really happy with the results. The main problems I found were:\n",
        "* The model will answer incorectly to the question\n",
        "* The model will answer to more question than the one answered\n",
        "* The model will stop suddently in the middle of a phrase\n",
        "\n",
        "## Using gemma_instruct_2b_en\n",
        "\n",
        "Then, I got a good advice in the comments from one of the Kagglers (thank you @paultimothymooney) and switched to `gemma_instruct_2b_en` with much better results.\n",
        "\n",
        "Here is the analysis of results with `gemma_instruct_2b_en`:\n",
        "* General question: answer to the point, and the answer seems sensible. Well, it is actually not entirely correct, but it is close. And the question did not clarify if it is during solar exposure or in the shadow.\n",
        "* Math problem, with answer as Python code: correct, and code is documented.\n",
        "* Multi-parameters question: all answers are correct.\n",
        "* More difficult math problem, requiring reasoning: the answer is not correct, because the reasoning is not correct but, to be fair, ... the problem contains a paradox. Did you (intelligent human reading this) notice what is the paradox?\n"
      ],
      "metadata": {
        "id": "QlESJbF1EsD1"
      }
    }
  ]
}