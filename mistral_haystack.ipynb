{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efz83OboW1lU"
      },
      "source": [
        "# Mistral + Haystack: build RAG pipelines that rock ðŸ¤˜\n",
        "\n",
        "### Unofficial experiment in which I build a Retrieval Augmented Generation pipeline for Rock music, using the new powerful [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) and [Haystack](https://github.com/deepset-ai/haystack) LLM orchestration framework.\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/anakin87/mistral-haystack/blob/main/mistral_haystack.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"200\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "<img src=\"https://techcrunch.com/wp-content/uploads/2023/09/mistral-7b-v0.1.jpg\" width=\"270\" style=\"display:inline;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=\"https://img.freepik.com/premium-vector/electric-guitar-fire-hot-rock-music-guitar-flames-hard-rock-rock-roll-concert-festival-label-night-club-live-show-vector-logo-emblem_570429-23178.jpg?w=2000\" width=\"180\"><img src=\"https://haystack.deepset.ai/images/haystack-ogimage.png\" width=\"360\" style=\"display:inline;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvH-QyVGasOv"
      },
      "source": [
        "## Install Haystack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWmrYElAayTl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install farm-haystack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5BUgeDfcLPI"
      },
      "source": [
        "## Load Mistral 7b in Haystack\n",
        "\n",
        "For simplicity, I am going to use the [Hugging Face Inference API](https://docs.haystack.deepset.ai/docs/prompt_node#using-hugging-face-inference-api), so we need a free Hugging Face Access Token (https://huggingface.co/settings/tokens).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIWRabo7dEaI"
      },
      "outputs": [],
      "source": [
        "from haystack.nodes import PromptNode\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naBfUr87jDfU",
        "outputId": "c86aebfb-98ba-4a04-8a05-e13a1db95dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your Hugging Face TokenÂ·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "HF_TOKEN = getpass(\"Your Hugging Face Token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtvDMWJ6dKsM"
      },
      "outputs": [],
      "source": [
        "pn = PromptNode(model_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.1\",  # instruct fine-tuned model: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
        "                max_length=800,\n",
        "                api_key=HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_25rnJyhYM-",
        "outputId": "0215a6a1-e4d8-438c-d99d-4897c5491101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Oh, absolutely! Large Language Models are the epitome of rock and roll. They're like the ultimate supergroup, with members from all walks of life and backgrounds, coming together to create something truly magical.\n",
            "\n",
            "First of all, they're incredibly versatile. They can handle any topic or question you throw their way, from pop culture to politics to philosophy. They're like the ultimate jukebox, always ready to play whatever song you want to hear.\n",
            "\n",
            "And let's not forget about their ability to learn and adapt. They're constantly evolving, taking in new information and refining their responses. It's like they're always on the cutting edge of the music scene, always pushing the boundaries of what's possible.\n",
            "\n",
            "But perhaps the most impressive thing about Large Language Models is their sheer size. They're like a giant machine, churning out hit after hit, never stopping until they've exhausted every last bit of their creativity.\n",
            "\n",
            "So yeah, in a completely ironic way, Large Language Models rock! They're the ultimate rock and roll supergroup, always ready to take on any challenge and deliver the goods.\n"
          ]
        }
      ],
      "source": [
        "# Let's quickly try the model\n",
        "\n",
        "out=pn(\"<s>[INST] Explain in a ironic way why Large Language Model rock! [/INST]\")\n",
        "\n",
        "print(out[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc9_DvoNlCQv"
      },
      "source": [
        "## Load and prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv2BeRdWa8zY"
      },
      "source": [
        "I load the data (Wikipedia pages about Rock) from an old project of mine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGdu3ObCbx-B",
        "outputId": "2eb6ff20-d83c-4b52-8300-e1144595e4b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'fact-checking-rocks'...\n",
            "remote: Enumerating objects: 319, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 319 (delta 21), reused 22 (delta 9), pack-reused 277\u001b[K\n",
            "Receiving objects: 100% (319/319), 77.79 KiB | 553.00 KiB/s, done.\n",
            "Resolving deltas: 100% (186/186), done.\n",
            "Filtering content: 100% (4/4), 222.56 MiB | 44.02 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://huggingface.co/spaces/anakin87/fact-checking-rocks\n",
        "! tar -xzf /content/fact-checking-rocks/data/rock_wiki.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaO7hy-wlbTL",
        "outputId": "1a1837f3-1117-45e2-e0b9-d15a4140a165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content : Robert Anthony Plant  (born 20 August 1948) is an English singer and songwriter, best known as the lead singer and lyricist of the English rock band Led Zeppelin for all of its existence from 1968 until 1980, when the band broke up following the deat\n",
            "meta : {'name': 'Robert Plant', 'url': 'https://en.wikipedia.org/wiki/Robert_Plant'}\n"
          ]
        }
      ],
      "source": [
        "# they are JSON file with a content field and some metadata\n",
        "import json\n",
        "\n",
        "with open(\"./rock_wiki/100405.json\") as f:\n",
        "    doc = json.load(f)\n",
        "\n",
        "for key, value in doc.items():\n",
        "    print(key, \":\", str(value)[:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsH7sSlzlVIa"
      },
      "source": [
        "I load these files into Haystack Documents, then I chunk them using a [Preprocessor](https://docs.haystack.deepset.ai/docs/preprocessor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Spblp58SW0nH"
      },
      "outputs": [],
      "source": [
        "import glob,json\n",
        "from haystack import Document\n",
        "from haystack.nodes import PreProcessor\n",
        "\n",
        "docs = []\n",
        "\n",
        "for json_file in glob.glob(\"/content/rock_wiki/*.json\"):\n",
        "    with open(json_file, \"r\") as fin:\n",
        "        doc_json = json.load(fin)\n",
        "    doc = Document.from_json(doc_json)\n",
        "\n",
        "    docs.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gZh5nXkmY1S",
        "outputId": "99a0085f-2656-434a-f6d8-cb0c42160f55"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Preprocessing:   3%|â–Ž         | 13/453 [00:00<00:07, 56.09docs/s]WARNING:haystack.nodes.preprocessor.preprocessor:We found one or more sentences whose word count is higher than the split length.\n",
            "Preprocessing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 453/453 [00:11<00:00, 38.18docs/s]\n"
          ]
        }
      ],
      "source": [
        "processor = PreProcessor(\n",
        "    clean_empty_lines=True,\n",
        "    clean_whitespace=True,\n",
        "    clean_header_footer=True,\n",
        "    split_by=\"word\",\n",
        "    split_length=200,\n",
        "    split_respect_sentence_boundary=True,\n",
        "    split_overlap=0,\n",
        "    language=\"en\",\n",
        ")\n",
        "preprocessed_docs = processor.process(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnMZhmSqmzjU",
        "outputId": "be0f8e30-6731-4803-b89a-8a862bb61d15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Document: {'content': 'Stephen Lawrence Winwood (born 12 May 1948) is an English professional musician and songwriter whose genres include blue-eyed soul, rhythm and blues, blues rock and pop rock. Though primarily a keyboard player and vocalist prominent for his distinctive, soulful high tenor voice, Winwood plays other instruments proficiently, including drums, mandolin, guitars, bass and saxophone.\\nWinwood was an integral member of three seminal musical ensembles of the 1960s and 1970s: the Spencer Davis Group, Traffic, and Blind Faith. Beginning in the 1980s, his solo career flourished and he had a number of hit singles, including \"While You See a Chance\" (1980) from the album Arc of a Diver and \"Valerie\" (1982) from Talking Back to the Night (\"Valerie\" became a hit when it was re-released with a remix from Winwood\\'s 1987 compilation album Chronicles). His 1986 album Back in the High Life marked his career zenith, with hit singles including \"Back in the High Life Again\", \"The Finer Things\" and the US Billboard Hot 100 number one hit \"Higher Love\". He found the top of the Hot 100 again with \"Roll With It\" (1988) from the album of the same name, with \"Holding On\" also charting highly the same year. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'Steve Winwood', 'url': 'https://en.wikipedia.org/wiki/Steve_Winwood', '_split_id': 0}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5610f221bc8281320b8a2cfb71001d0e'}>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# a smaller chunked document\n",
        "\n",
        "preprocessed_docs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6M7pttrnrQI"
      },
      "source": [
        "### Create an InMemoryDocumentStore and store data\n",
        "\n",
        "It's a very simple Document Store, ideal for rapid prototyping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40jmeS34m_bM"
      },
      "outputs": [],
      "source": [
        "from haystack.document_stores import InMemoryDocumentStore\n",
        "\n",
        "document_store = InMemoryDocumentStore(use_bm25=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7tnuZBeoIqs",
        "outputId": "822aaf7b-9ad0-4014-b455-2bf71416617d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Updating BM25 representation...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13236/13236 [00:01<00:00, 12001.50 docs/s]\n"
          ]
        }
      ],
      "source": [
        "document_store.write_documents(preprocessed_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpCA8b2FoZE2"
      },
      "source": [
        "## Create a RAG Pipeline\n",
        "\n",
        "It will be composed by:\n",
        "- a [BM25 Retriever](https://docs.haystack.deepset.ai/docs/retriever#bm25-recommended) ðŸ”Ž to find relevant Documents\n",
        "- a [PromptNode](https://docs.haystack.deepset.ai/docs/prompt_node) ðŸ¤– with our LLM to generate grounded answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UchlKzipUZf"
      },
      "outputs": [],
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.nodes import BM25Retriever, PromptNode, PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9_m3jz1pewv"
      },
      "outputs": [],
      "source": [
        "retriever = BM25Retriever(document_store, top_k=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKStelv4pr_f"
      },
      "outputs": [],
      "source": [
        "# a good Question Answering template, adapted for the instruction format\n",
        "# (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)\n",
        "\n",
        "qa_template = PromptTemplate(prompt=\n",
        "  \"\"\"<s>[INST] Using the information contained in the context, answer the question (using a maximum of two sentences).\n",
        "  If the answer cannot be deduced from the context, answer \\\"I don't know.\\\"\n",
        "  Context: {join(documents)};\n",
        "  Question: {query}\n",
        "  [/INST]\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7M8_QmfqGO7"
      },
      "outputs": [],
      "source": [
        "prompt_node = PromptNode(model_name_or_path=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "                         api_key=HF_TOKEN,\n",
        "                         default_prompt_template=qa_template,\n",
        "                         max_length=5500,\n",
        "                         model_kwargs={\"model_max_length\":8000})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzD5jw84oTVr"
      },
      "outputs": [],
      "source": [
        "rag_pipeline = Pipeline()\n",
        "rag_pipeline.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
        "rag_pipeline.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"retriever\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHzz1UQlt1M5"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "print_answer = lambda out: pprint(out[\"results\"][0].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VXJ2W7ju9c9"
      },
      "source": [
        "## Let's try our RAG Pipeline ðŸŽ¸\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua-bETg0qknO",
        "outputId": "df9bab37-1e27-4d70-f18f-e7bd0aa33b5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Elvis Presley was a legendary American singer, actor, and musician, known '\n",
            " 'for his significant impact on the popularization of rock and roll. He was '\n",
            " 'born on January 8, 1935, in Tupelo, Mississippi, and had a close bond with '\n",
            " 'his mother. His father was of German, Scottish, and English origins, while '\n",
            " \"his mother was of Scots-Irish with some French Norman ancestry. Elvis's \"\n",
            " 'mother believed that her great-great-grandmother, Morning Dove White, was '\n",
            " \"Cherokee. Elvis's early years were marked by his love for music, which he \"\n",
            " 'discovered at an Assembly of God church.')\n"
          ]
        }
      ],
      "source": [
        "print_answer(rag_pipeline.run(query=\"Who was Elvis Presley?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lq481NPqmVj",
        "outputId": "9948e90f-0b1c-44d2-d7dd-93c3a685fd1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'The initial name of Sum 41 was Supernova.'\n"
          ]
        }
      ],
      "source": [
        "print_answer(rag_pipeline.run(query=\"What was the initial name of Sum 41?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1vomcb7q5E8",
        "outputId": "354ead3f-80ca-4998-b141-ad94dcf0c8ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('No, Ozzy Osbourne is not a member of Blink-182. He is a member of Black '\n",
            " 'Sabbath.')\n"
          ]
        }
      ],
      "source": [
        "print_answer(rag_pipeline.run(query=\"Is Ozzy Osbourne a member of Blink 182?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHz1FeQgsYj6",
        "outputId": "827d5cee-ddb7-4465-a7df-f56801c0321f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(\"I don't know. The provided context does not mention anything about the Earth \"\n",
            " 'being flat.')\n",
            "(\"I don't know. The context does mention Louis Jordan, but it does not provide \"\n",
            " 'any information about a person named Michael Jordan.')\n"
          ]
        }
      ],
      "source": [
        "# let's try to fool the model\n",
        "\n",
        "print_answer(rag_pipeline.run(query=\"Is the earth flat?\"))\n",
        "\n",
        "print_answer(rag_pipeline.run(query=\"Who is Michael Jordan?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7dzb2qN0vEi"
      },
      "source": [
        "## Try it for yourself!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afhwhAIivK1P"
      },
      "outputs": [],
      "source": [
        "questions=\"\"\"Who is Lady Gaga?\n",
        "What is Nirvana's first album?\n",
        "Where System of a down come from?\n",
        "Who wrote the song Stairway to heaven?\n",
        "Can you tell me the names of some hard rock bands?\n",
        "How many albums have Faith No More release?\n",
        "Why are Sex Pistols so popular, even though their career was short?\"\"\".split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AACGqGFOw2AE"
      },
      "outputs": [],
      "source": [
        "for q in questions:\n",
        "  print(\"\\n\"+q)\n",
        "  print_answer(rag_pipeline.run(query=q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCY4GhnY09oE"
      },
      "source": [
        "## More (generative) ideas with Haystack\n",
        "- [Customizing PromptNode for NLP Tasks](https://haystack.deepset.ai/tutorials/21_customizing_promptnode/)\n",
        "- [Creating a Generative QA Pipeline with PromptNode](https://haystack.deepset.ai/tutorials/22_pipeline_with_promptnode)\n",
        "- [Answering Multihop Questions with Agents](https://haystack.deepset.ai/tutorials/23_answering_multihop_questions_with_agents)\n",
        "- [Building a Conversational Chat App](https://haystack.deepset.ai/tutorials/24_building_chat_app/)\n",
        "- [Customizing Agent to Chat with Your Documents](https://haystack.deepset.ai/tutorials/25_customizing_agent)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pc9_DvoNlCQv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}