{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        },
        {
          "sourceId": 7705679,
          "sourceType": "datasetVersion",
          "datasetId": 4498747
        },
        {
          "sourceId": 11371,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 5171
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Google AI Assistant for Data Science teaching",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashispapu/LLMs/blob/main/Google_AI_Assistant_for_Data_Science_teaching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'data-science-interview-q-and-a-treasury:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4498747%2F7705679%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240403%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240403T082644Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D14cdf030232eb9e8c603c85a227afe99d4ef8ae28c816dffa137f5be86e4f4bfadd374e651b5cbd6a0961e6c72926c78edfdc10946e20761ae8e84050dcf195313bf081960d3702f1f5fa16796f788b93bcf70311889bc72808256b6737e5b64e5f7923347b5582d0d6cd52b09cb69207684bcb8d6c33c73e5cd1025c13e36fd3c1eca42ab7b59876af1bc0dca35b4d3de1159be2b1029f13646629cc7c6ea7759c6c2a421c4b04896cdadce607d8db98906021443584de73455832157f2a888a9bf822c4471622ea5d2fc9345edca7c54cdc44f1a00da1d353685776f0783f155212e78bb8f30fc7ca414163193351cc33ad6f2f67f9346d284107d6a84d894'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "-yYV3t47EWAL"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1>Google AI Assistant for Data Science teaching</h1></center>\n",
        "\n",
        "<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This Notebook will build an AI Assistant for Data Science teaching.\n",
        "\n",
        "## The method\n",
        "\n",
        "We will fine-tune a LLM with Data Science questions and answers. Then we will create a custom class that will query this model.\n",
        "\n",
        "## The model\n",
        "\n",
        "As model, we will use Gemma model, fine-tuned with our Data Science Q&A data, using LoRA.\n",
        "\n",
        "## The data\n",
        "\n",
        "As data for fine-tuning Gemma, we will use [Data Science Q&A Treasury](https://www.kaggle.com/datasets/memocan/data-science-interview-q-and-a-treasury) dataset. This dataset contains over 150 questions and answering about Data Science.\n",
        "\n",
        "## Previous work\n",
        "\n",
        "This work is largely based on previous work. Here I list the sources:\n",
        "\n",
        "1. Gemma Model Card, Kaggle Models, https://www.kaggle.com/models/google/gemma\n",
        "2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)\n",
        "3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1)\n",
        "4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n",
        "5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n",
        "6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "1qQxWt44EWAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction about Gemma\n",
        "\n",
        "\n",
        "Gemma is a collection of lightweight source generative AI models designed to be used mostly by developers and researchers. Created by Google DeepMind research lab that also developed Gemini, Gemma is available in several versions, with 2B and 7B parameters, as following:\n",
        "\n",
        "| Model                  | Parameters      | Tuned versions    | Description                                    | Recomemnded target platforms       |\n",
        "|------------------------|-----------------|-------------------|------------------------------------------------|------------------------------------|\n",
        "| `gemma_2b_en`          | 2.51B           | Pretrained        | 18-layer Gemma model (Gemma with 2B parameters)|Mobile devices and laptops          |\n",
        "| `gemma_instruct_2b_en` | 2.51B           | Instruction tuned | 18-layer Gemma model (Gemma with 2B parameters)| Mobile devices and laptops         |\n",
        "| `gemma_7b_en`          | 8.54B           | Pretrained        | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
        "| `gemma_instruct_7b_en` | 8.54B           | Instruction tuned | 28-layer Gemma model (Gemma with 7B parameters)| Desktop computers and small servers|\n",
        "\n",
        "\n",
        "For this notebook, we will fine-tune `gemma_2b_en` model, one of the `2B` parameters Gemma models."
      ],
      "metadata": {
        "id": "P9wXHJjrEWAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA introduction\n",
        "\n",
        "LoRA stands for Low-Rank Adaptation. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices.   \n",
        "The number of trainable parameters during fine-tunning will decrease therefore considerably.   \n",
        "According to LoRA paper, this number decreases 10,000 times, and the computational resources size decreases 3 times."
      ],
      "metadata": {
        "id": "1UGWDY4bEWAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and configurations"
      ],
      "metadata": {
        "id": "Yoi1rdc6EWAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U keras>=3"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T14:28:15.940895Z",
          "iopub.execute_input": "2024-04-01T14:28:15.941654Z",
          "iopub.status.idle": "2024-04-01T14:28:46.212886Z",
          "shell.execute_reply.started": "2024-04-01T14:28:15.941611Z",
          "shell.execute_reply": "2024-04-01T14:28:46.211784Z"
        },
        "trusted": true,
        "id": "IYE3XV4KEWAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
        "\n",
        "import keras\n",
        "import keras_nlp\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas() # progress bar for pandas\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T14:28:46.214941Z",
          "iopub.execute_input": "2024-04-01T14:28:46.215252Z",
          "iopub.status.idle": "2024-04-01T14:29:00.566403Z",
          "shell.execute_reply.started": "2024-04-01T14:28:46.215224Z",
          "shell.execute_reply": "2024-04-01T14:29:00.565605Z"
        },
        "trusted": true,
        "id": "3ScC0wbuEWAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    seed = 42\n",
        "    dataset_path = \"/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv\"\n",
        "    preset = \"gemma_2b_en\" # name of pretrained Gemma\n",
        "    sequence_length = 512 # max size of input sequence for training\n",
        "    batch_size = 1 # size of the input batch in training, x 2 as two GPUs\n",
        "    epochs = 10 # number of epochs to train"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T14:29:00.567594Z",
          "iopub.execute_input": "2024-04-01T14:29:00.568527Z",
          "iopub.status.idle": "2024-04-01T14:29:00.573498Z",
          "shell.execute_reply.started": "2024-04-01T14:29:00.568492Z",
          "shell.execute_reply": "2024-04-01T14:29:00.57249Z"
        },
        "trusted": true,
        "id": "7OMxSReFEWAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.set_random_seed(Config.seed)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T14:29:00.574785Z",
          "iopub.execute_input": "2024-04-01T14:29:00.575145Z",
          "iopub.status.idle": "2024-04-01T14:29:00.608104Z",
          "shell.execute_reply.started": "2024-04-01T14:29:00.575099Z",
          "shell.execute_reply": "2024-04-01T14:29:00.607232Z"
        },
        "trusted": true,
        "id": "eq8t50HIEWAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def colorize_text(text):\n",
        "    for word, color in zip([\"Question\", \"Answer\"], [\"blue\", \"red\"]):\n",
        "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
        "    return text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T14:29:00.610342Z",
          "iopub.execute_input": "2024-04-01T14:29:00.610673Z",
          "iopub.status.idle": "2024-04-01T14:29:00.619607Z",
          "shell.execute_reply.started": "2024-04-01T14:29:00.61063Z",
          "shell.execute_reply": "2024-04-01T14:29:00.618789Z"
        },
        "trusted": true,
        "id": "tsqcFnqTEWAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the data"
      ],
      "metadata": {
        "id": "nRTHQIx3EWAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(f\"{Config.dataset_path}\")\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T14:29:00.620746Z",
          "iopub.execute_input": "2024-04-01T14:29:00.621523Z",
          "iopub.status.idle": "2024-04-01T14:29:00.655958Z",
          "shell.execute_reply.started": "2024-04-01T14:29:00.621499Z",
          "shell.execute_reply": "2024-04-01T14:29:00.655141Z"
        },
        "trusted": true,
        "id": "GIgZWDyMEWAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune Gemma\n",
        "\n",
        "We prepare a template and generate a prompt using this template from each row in the dataset."
      ],
      "metadata": {
        "id": "mh38wcglEWAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\\n\\nQuestion:\\n{question}\\n\\nAnswer:\\n{answer}\"\n",
        "df[\"prompt\"] = df.progress_apply(lambda row: template.format(question=row.question,\n",
        "                                                             answer=row.answer), axis=1)\n",
        "data = df.prompt.tolist()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T14:29:00.657192Z",
          "iopub.execute_input": "2024-04-01T14:29:00.657731Z",
          "iopub.status.idle": "2024-04-01T14:29:00.687381Z",
          "shell.execute_reply.started": "2024-04-01T14:29:00.6577Z",
          "shell.execute_reply": "2024-04-01T14:29:00.68528Z"
        },
        "trusted": true,
        "id": "Ky4rx6_cEWAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the code for Gemma Causal LM\n",
        "\n",
        "We initialize `GemmaCausalML` with `gemma_2b_en` model."
      ],
      "metadata": {
        "id": "w_3ROXxEEWAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
        "gemma_lm.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T14:29:00.688609Z",
          "iopub.execute_input": "2024-04-01T14:29:00.688899Z",
          "iopub.status.idle": "2024-04-01T14:30:16.077553Z",
          "shell.execute_reply.started": "2024-04-01T14:29:00.688857Z",
          "shell.execute_reply": "2024-04-01T14:30:16.076612Z"
        },
        "trusted": true,
        "id": "SbyzUJYfEWAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemma preprocessor"
      ],
      "metadata": {
        "id": "tb2NnP_NEWAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y, sample_weight = gemma_lm.preprocessor(data[0:1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T14:30:16.078777Z",
          "iopub.execute_input": "2024-04-01T14:30:16.079063Z",
          "iopub.status.idle": "2024-04-01T14:30:16.404722Z",
          "shell.execute_reply.started": "2024-04-01T14:30:16.079037Z",
          "shell.execute_reply": "2024-04-01T14:30:16.40393Z"
        },
        "trusted": true,
        "id": "0-EmzpH4EWAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enable LoRA for the model\n",
        "\n",
        "We set LoRA rank to 5. The higher the LoRA rank, the higher the number of trainable parameters."
      ],
      "metadata": {
        "id": "3GnNPbbvEWAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable LoRA for the model and set the LoRA rank to 5.\n",
        "gemma_lm.backbone.enable_lora(rank=5)\n",
        "gemma_lm.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T14:30:16.405792Z",
          "iopub.execute_input": "2024-04-01T14:30:16.40608Z",
          "iopub.status.idle": "2024-04-01T14:30:16.858186Z",
          "shell.execute_reply.started": "2024-04-01T14:30:16.406056Z",
          "shell.execute_reply": "2024-04-01T14:30:16.857007Z"
        },
        "trusted": true,
        "id": "VtOaSuorEWAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total trainable parameters is 1.7 M (or 6.5 MB).  \n",
        "This is less than 0.06% of the 2,5G (9.35 GB) total trainable parameters."
      ],
      "metadata": {
        "id": "6TNCNCWyEWAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the fine-tuning sequence"
      ],
      "metadata": {
        "id": "YNsA7jvgEWAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the input sequence length to 512 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = Config.sequence_length\n",
        "\n",
        "# Compile the model with loss, optimizer, and metric\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=8e-5),\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train model\n",
        "gemma_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-01T14:26:03.457462Z",
          "iopub.execute_input": "2024-04-01T14:26:03.457824Z"
        },
        "trusted": true,
        "id": "PAQqFfTpEWAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the fine-tuned model"
      ],
      "metadata": {
        "id": "gTEluvLqEWAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the specialized class"
      ],
      "metadata": {
        "id": "vxu481vTEWAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaQA:\n",
        "    def __init__(self, max_length=512):\n",
        "        self.max_length = max_length\n",
        "        self.prompt = template\n",
        "        self.gemma_lm = gemma_lm\n",
        "\n",
        "    def query(self, question):\n",
        "        response = self.gemma_lm.generate(\n",
        "            self.prompt.format(\n",
        "                question=question,\n",
        "                answer=\"\"),\n",
        "            max_length=self.max_length)\n",
        "        display(Markdown(colorize_text(response)))\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T21:39:17.871079Z",
          "iopub.execute_input": "2024-03-31T21:39:17.871471Z",
          "iopub.status.idle": "2024-03-31T21:39:17.877575Z",
          "shell.execute_reply.started": "2024-03-31T21:39:17.871439Z",
          "shell.execute_reply": "2024-03-31T21:39:17.87654Z"
        },
        "trusted": true,
        "id": "b3qHIKHXEWAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_qa = GemmaQA()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T21:39:19.898528Z",
          "iopub.execute_input": "2024-03-31T21:39:19.899353Z",
          "iopub.status.idle": "2024-03-31T21:39:19.903296Z",
          "shell.execute_reply.started": "2024-03-31T21:39:19.899323Z",
          "shell.execute_reply": "2024-03-31T21:39:19.902273Z"
        },
        "trusted": true,
        "id": "6w3yKqobEWAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 1"
      ],
      "metadata": {
        "id": "tzUwe4A-EWAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row = df.iloc[5]\n",
        "gemma_qa.query(row.question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T21:40:25.477412Z",
          "iopub.execute_input": "2024-03-31T21:40:25.478303Z",
          "iopub.status.idle": "2024-03-31T21:40:33.377444Z",
          "shell.execute_reply.started": "2024-03-31T21:40:25.478269Z",
          "shell.execute_reply": "2024-03-31T21:40:33.376342Z"
        },
        "trusted": true,
        "id": "7NAk_XJFEWAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 2"
      ],
      "metadata": {
        "id": "O67grHjUEWAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row = df.iloc[10]\n",
        "gemma_qa.query(row.question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T21:40:41.516077Z",
          "iopub.execute_input": "2024-03-31T21:40:41.516439Z",
          "iopub.status.idle": "2024-03-31T21:40:53.382549Z",
          "shell.execute_reply.started": "2024-03-31T21:40:41.516411Z",
          "shell.execute_reply": "2024-03-31T21:40:53.38155Z"
        },
        "trusted": true,
        "id": "9YkOeYtrEWAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 3"
      ],
      "metadata": {
        "id": "bCP3tOtPEWAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row = df.iloc[15]\n",
        "gemma_qa.query(row.question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-31T21:40:53.384394Z",
          "iopub.execute_input": "2024-03-31T21:40:53.385273Z",
          "iopub.status.idle": "2024-03-31T21:41:05.589108Z",
          "shell.execute_reply.started": "2024-03-31T21:40:53.385236Z",
          "shell.execute_reply": "2024-03-31T21:41:05.588114Z"
        },
        "trusted": true,
        "id": "3H9UEj-1EWAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fresh question 1"
      ],
      "metadata": {
        "id": "67d3eAQBEWAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is regularization?\"\n",
        "gemma_qa.query(question)"
      ],
      "metadata": {
        "id": "lZw7wCP2EWAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fresh question 2"
      ],
      "metadata": {
        "id": "VkAEUrhiEWAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is SVM?\"\n",
        "gemma_qa.query(question)"
      ],
      "metadata": {
        "id": "oL1Nd855EWAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fresh question 3"
      ],
      "metadata": {
        "id": "hSW72nF9EWAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is Dropout?\"\n",
        "gemma_qa.query(question)"
      ],
      "metadata": {
        "id": "hTm2P2j8EWAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "\n",
        "We fine-tuned Gemma with a set of Data Science interview question and answers.   \n",
        "Then we tested the model with questions from the dataset used for fine-tuning.  \n",
        "At the end, we also tested with some questions that were not in the dataset."
      ],
      "metadata": {
        "id": "z_lhJNKgEWA0"
      }
    }
  ]
}